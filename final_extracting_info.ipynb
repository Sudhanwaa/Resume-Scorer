{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdf2image'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32md:\\Technolution_Assignmnet\\final_extracting_info.ipynb Cell 1\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Technolution_Assignmnet/final_extracting_info.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m  \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Technolution_Assignmnet/final_extracting_info.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Technolution_Assignmnet/final_extracting_info.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpdf2image\u001b[39;00m \u001b[39mimport\u001b[39;00m convert_from_path\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Technolution_Assignmnet/final_extracting_info.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytesseract\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Technolution_Assignmnet/final_extracting_info.ipynb#W0sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pdf2image'"
     ]
    }
   ],
   "source": [
    "import pandas as  pd\n",
    "import PIL\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import spacy\n",
    "import re\n",
    "from contractions import fix\n",
    "from nltk.corpus import stopwords\n",
    "import fitz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sudha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEFINING SPECIFIC FUNCTIONS TO CARRY OUT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    lowercased_text = text.lower()\n",
    "    return lowercased_text\n",
    "\n",
    "def removing_stopwords(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
    "    return filtered_tokens\n",
    "\n",
    "def removing_special_characters(text):\n",
    "    # Check if text is None or not a string\n",
    "    if text is None or not isinstance(text, str):\n",
    "        return text  # Return input unchanged\n",
    "\n",
    "    # Use regex to remove special characters\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def lemmatization(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    lemmatized_text = \" \".join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING ALL THE TEXT FROM THE RESUME\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_number in range(doc.page_count):\n",
    "        page = doc[page_number]\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "path=\"Sudhanwa_Bokade_ML_Resume.pdf\"\n",
    "resume_text_original=extract_text_from_pdf(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING RELEVANT LINKS FROM A RESUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Links': [], 'Emails': ['sudhanwaofficial@gmail.com']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_links_and_emails(resume_text):\n",
    "    # Define regex patterns for matching URLs and email addresses\n",
    "    link_pattern = r\"(https?://[^\\s]+)\"\n",
    "    email_pattern = r\"[\\w\\.-]+@[\\w\\.-]+\"\n",
    "\n",
    "    # Use re.findall to find all matches of the patterns in the text\n",
    "    links = re.findall(link_pattern, resume_text)\n",
    "    emails = re.findall(email_pattern, resume_text)\n",
    "\n",
    "    return {\"Links\": links, \"Emails\": emails}\n",
    "\n",
    "resume_text=to_lowercase(resume_text_original)\n",
    "resume_text=lemmatization(resume_text)\n",
    "extract_links_and_emails(resume_text)\n",
    "# NOT TAKING INTO CONSIDERATION removing_special_characters SINCE IT MAY REMOVE IMPORTANT LINKS\n",
    "# NOT TAKING removing_stopwords INTO CONSIDERATION BECAUSE IT MAY DECREASE THE IMPORTANCE OF THE IMPORTANT KEYWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING IMPORTANT SKILLS FROM RESUME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pytorch',\n",
       " 'Training',\n",
       " 'Healthcare',\n",
       " 'Website',\n",
       " 'System',\n",
       " 'Github',\n",
       " 'Audio',\n",
       " 'Forecasting',\n",
       " 'Research',\n",
       " 'Java',\n",
       " 'Analysis',\n",
       " 'Python',\n",
       " 'Health',\n",
       " 'Cloud',\n",
       " 'Javascript',\n",
       " 'C',\n",
       " 'Ai',\n",
       " 'Analyze',\n",
       " 'Tensorflow',\n",
       " 'Certification',\n",
       " 'Architecture',\n",
       " 'Css',\n",
       " 'Html',\n",
       " 'Flask',\n",
       " 'Engineering',\n",
       " 'Teaching',\n",
       " 'Programming',\n",
       " 'C++',\n",
       " 'Access',\n",
       " 'Postgresql',\n",
       " 'Technical',\n",
       " 'Numpy',\n",
       " 'Process']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "# noun_chunks = nlp.noun_chunks\n",
    "\n",
    "def extract_skills(resume_text):\n",
    "    nlp_text = nlp(resume_text)\n",
    "\n",
    "    # removing stop words and implementing word tokenization\n",
    "    tokens = [token.text for token in nlp_text if not token.is_stop]\n",
    "    \n",
    "    # reading the csv file\n",
    "    data = pd.read_csv(\"skills.csv\") \n",
    "    \n",
    "    # extract values\n",
    "    skills = list(data.columns.values)\n",
    "    \n",
    "    skillset = []\n",
    "    \n",
    "    # check for one-grams (example: python)\n",
    "    for token in tokens:\n",
    "        if token.lower() in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    # check for bi-grams and tri-grams (example: machine learning)\n",
    "    for token in nlp_text.noun_chunks:\n",
    "        token = token.text.lower().strip()\n",
    "        if token in skills:\n",
    "            skillset.append(token)\n",
    "    \n",
    "    return [i.capitalize() for i in set([i.lower() for i in skillset])]\n",
    "\n",
    "\n",
    "resume_text=to_lowercase(resume_text_original)\n",
    "resume_text=removing_stopwords(resume_text)\n",
    "resume_text=removing_special_characters(resume_text)\n",
    "resume_text=lemmatization(' '.join(resume_text))\n",
    "extract_skills(resume_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACTING IMPORTANT CERTIFICATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_certifications(resume_text, required_certifications):\n",
    "    certification_pattern = r\"([A-Za-z\\s]+) - (\\d{4})\"\n",
    "    certifications = re.findall(certification_pattern, resume_text)\n",
    "    \n",
    "    # Compare extracted certifications with required certifications\n",
    "    matching_certifications = [cert for cert in certifications if cert[0] in required_certifications]\n",
    "    \n",
    "    return matching_certifications\n",
    "\n",
    "# Example usage\n",
    "required_certifications = [\"PMP\", \"AWS Certified\"]\n",
    "result = extract_certifications(resume_text, required_certifications)\n",
    "print(\"Matching Certifications:\", result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
